<!DOCTYPE html>

<html :class="{'dark': darkMode === 'dark' || (darkMode === 'system' &amp;&amp; window.matchMedia('(prefers-color-scheme: dark)').matches)}" class="scroll-smooth" lang="en" x-data="{ darkMode: localStorage.getItem('darkMode') || localStorage.setItem('darkMode', 'system'), activeSection: '' }" x-init="$watch('darkMode', val =&gt; localStorage.setItem('darkMode', val))">
<head>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta charset="utf-8"/>
<meta content="white" media="(prefers-color-scheme: light)" name="theme-color"/>
<meta content="black" metia="(prefers-color-scheme: dark)" name="theme-color"/>
<meta content="Docutils 0.18.1: http://docutils.sourceforge.net/" name="generator"/>
<title>Introduction to Parallel Programming | UnoAPI: updated v0.3.1</title>
<meta content="Introduction to Parallel Programming | UnoAPI: updated v0.3.1" property="og:title"/>
<meta content="Introduction to Parallel Programming | UnoAPI: updated v0.3.1" name="twitter:title"/>
<link href="../_static/pygments.css" rel="stylesheet"/>
<link href="../_static/theme.47abdafeea10b5ccf58b.css" rel="stylesheet"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../10-software-engineering/software-engineering.html" rel="next" title="Introduction to Software Engineering"/>
<link href="../00-preliminaries/preliminaries.html" rel="prev" title="About the Book"/>
<script>
    <!-- Prevent Flash of wrong theme -->
      const userPreference = localStorage.getItem('darkMode');
      let mode;
      if (userPreference === 'dark' || window.matchMedia('(prefers-color-scheme: dark)').matches) {
        mode = 'dark';
        document.documentElement.classList.add('dark');
      } else {
        mode = 'light';
      }
      if (!userPreference) {localStorage.setItem('darkMode', mode)}
    </script>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/sphinx_highlight.js"></script>
<script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script defer="" src="../_static/theme.4ea23184acb23c5e9294.js"></script>
</head>
<body :class="{ 'overflow-hidden': showSidebar }" class="min-h-screen font-sans antialiased bg-background text-foreground" x-data="{ showSidebar: false }">
<div @click.self="showSidebar = false" class="fixed inset-0 z-50 overflow-hidden bg-background/80 backdrop-blur-sm" x-cloak="" x-show="showSidebar"></div><div class="relative flex flex-col min-h-screen" id="page"><a class="absolute top-0 left-0 z-[100] block bg-background p-4 text-xl transition -translate-x-full opacity-0 focus:translate-x-0 focus:opacity-100" href="#content">
      Skip to content
    </a>
<header class="sticky top-0 z-40 w-full border-b shadow-sm border-border supports-backdrop-blur:bg-background/60 bg-background/95 backdrop-blur"><div class="container flex items-center h-14">
<div class="hidden mr-4 md:flex">
<a class="flex items-center mr-6" href="../index.html"><span class="hidden font-bold sm:inline-block text-clip whitespace-nowrap">UnoAPI: updated v0.3.1</span>
</a></div><button @click="showSidebar = true" class="inline-flex items-center justify-center h-10 px-0 py-2 mr-2 text-base font-medium transition-colors rounded-md hover:text-accent-foreground hover:bg-transparent md:hidden" type="button">
<svg aria-hidden="true" fill="currentColor" height="24" viewbox="0 96 960 960" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M152.587 825.087q-19.152 0-32.326-13.174t-13.174-32.326q0-19.152 13.174-32.326t32.326-13.174h440q19.152 0 32.326 13.174t13.174 32.326q0 19.152-13.174 32.326t-32.326 13.174h-440Zm0-203.587q-19.152 0-32.326-13.174T107.087 576q0-19.152 13.174-32.326t32.326-13.174h320q19.152 0 32.326 13.174T518.087 576q0 19.152-13.174 32.326T472.587 621.5h-320Zm0-203.587q-19.152 0-32.326-13.174t-13.174-32.326q0-19.152 13.174-32.326t32.326-13.174h440q19.152 0 32.326 13.174t13.174 32.326q0 19.152-13.174 32.326t-32.326 13.174h-440ZM708.913 576l112.174 112.174q12.674 12.674 12.674 31.826t-12.674 31.826Q808.413 764.5 789.261 764.5t-31.826-12.674l-144-144Q600 594.391 600 576t13.435-31.826l144-144q12.674-12.674 31.826-12.674t31.826 12.674q12.674 12.674 12.674 31.826t-12.674 31.826L708.913 576Z"></path>
</svg>
<span class="sr-only">Toggle navigation menu</span>
</button>
<div class="flex items-center justify-between flex-1 space-x-2 sm:space-x-4 md:justify-end">
<div class="flex-1 w-full md:w-auto md:flex-none">
<form @keydown.k.window.meta="$refs.search.focus()" action="../search.html" class="relative flex items-center group" id="searchbox" method="get">
<input aria-label="Search the docs" class="inline-flex items-center font-medium transition-colors bg-transparent focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 ring-offset-background border border-input hover:bg-accent focus:bg-accent hover:text-accent-foreground focus:text-accent-foreground hover:placeholder-accent-foreground py-2 px-4 relative h-9 w-full justify-start rounded-[0.5rem] text-sm text-muted-foreground sm:pr-12 md:w-40 lg:w-64" id="search-input" name="q" placeholder="Search ..." type="search" x-ref="search"/>
<kbd class="pointer-events-none absolute right-1.5 top-2 hidden h-5 select-none text-muted-foreground items-center gap-1 rounded border border-border bg-muted px-1.5 font-mono text-[10px] font-medium opacity-100 sm:flex group-hover:bg-accent group-hover:text-accent-foreground">
<span class="text-xs">⌘</span>
      K
    </kbd>
</form>
</div>
<nav class="flex items-center space-x-1">
<button @click="darkMode = darkMode === 'light' ? 'dark' : 'light'" class="relative inline-flex items-center justify-center px-0 text-sm font-medium transition-colors rounded-md hover:bg-accent hover:text-accent-foreground h-9 w-9" type="button">
<svg class="absolute transition-all scale-100 rotate-0 dark:-rotate-90 dark:scale-0" fill="currentColor" height="24" viewbox="0 96 960 960" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M480 685q45.456 0 77.228-31.772Q589 621.456 589 576q0-45.456-31.772-77.228Q525.456 467 480 467q-45.456 0-77.228 31.772Q371 530.544 371 576q0 45.456 31.772 77.228Q434.544 685 480 685Zm0 91q-83 0-141.5-58.5T280 576q0-83 58.5-141.5T480 376q83 0 141.5 58.5T680 576q0 83-58.5 141.5T480 776ZM80 621.5q-19.152 0-32.326-13.174T34.5 576q0-19.152 13.174-32.326T80 530.5h80q19.152 0 32.326 13.174T205.5 576q0 19.152-13.174 32.326T160 621.5H80Zm720 0q-19.152 0-32.326-13.174T754.5 576q0-19.152 13.174-32.326T800 530.5h80q19.152 0 32.326 13.174T925.5 576q0 19.152-13.174 32.326T880 621.5h-80Zm-320-320q-19.152 0-32.326-13.174T434.5 256v-80q0-19.152 13.174-32.326T480 130.5q19.152 0 32.326 13.174T525.5 176v80q0 19.152-13.174 32.326T480 301.5Zm0 720q-19.152 0-32.326-13.17Q434.5 995.152 434.5 976v-80q0-19.152 13.174-32.326T480 850.5q19.152 0 32.326 13.174T525.5 896v80q0 19.152-13.174 32.33-13.174 13.17-32.326 13.17ZM222.174 382.065l-43-42Q165.5 327.391 166 308.239t13.174-33.065q13.435-13.674 32.587-13.674t32.065 13.674l42.239 43q12.674 13.435 12.555 31.706-.12 18.272-12.555 31.946-12.674 13.674-31.445 13.413-18.772-.261-32.446-13.174Zm494 494.761-42.239-43q-12.674-13.435-12.674-32.087t12.674-31.565Q686.609 756.5 705.38 757q18.772.5 32.446 13.174l43 41.761Q794.5 824.609 794 843.761t-13.174 33.065Q767.391 890.5 748.239 890.5t-32.065-13.674Zm-42-494.761Q660.5 369.391 661 350.62q.5-18.772 13.174-32.446l41.761-43Q728.609 261.5 747.761 262t33.065 13.174q13.674 13.435 13.674 32.587t-13.674 32.065l-43 42.239q-13.435 12.674-31.706 12.555-18.272-.12-31.946-12.555Zm-495 494.761Q165.5 863.391 165.5 844.239t13.674-32.065l43-42.239q13.435-12.674 32.087-12.674t31.565 12.674Q299.5 782.609 299 801.38q-.5 18.772-13.174 32.446l-41.761 43Q231.391 890.5 212.239 890t-33.065-13.174ZM480 576Z"></path>
</svg>
<svg class="absolute transition-all scale-0 rotate-90 dark:rotate-0 dark:scale-100" fill="currentColor" height="24" viewbox="0 96 960 960" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M480 936q-151 0-255.5-104.5T120 576q0-138 90-239.5T440 218q25-3 39 18t-1 44q-17 26-25.5 55t-8.5 61q0 90 63 153t153 63q31 0 61.5-9t54.5-25q21-14 43-1.5t19 39.5q-14 138-117.5 229T480 936Zm0-80q88 0 158-48.5T740 681q-20 5-40 8t-40 3q-123 0-209.5-86.5T364 396q0-20 3-40t8-40q-78 32-126.5 102T200 576q0 116 82 198t198 82Zm-10-270Z"></path>
</svg>
</button>
</nav>
</div>
</div>
</header>
<div class="flex-1"><div class="container flex-1 items-start md:grid md:grid-cols-[220px_minmax(0,1fr)] md:gap-6 lg:grid-cols-[240px_minmax(0,1fr)] lg:gap-10"><aside :aria-hidden="!showSidebar" :class="{ 'translate-x-0': showSidebar }" class="fixed inset-y-0 left-0 md:top-14 z-50 md:z-30 bg-background md:bg-transparent transition-all duration-100 -translate-x-full md:translate-x-0 ml-0 p-6 md:p-0 md:-ml-2 md:h-[calc(100vh-3.5rem)] w-5/6 md:w-full shrink-0 overflow-y-auto border-r border-border md:sticky" id="left-sidebar">
<a class="!justify-start text-sm md:!hidden bg-background" href="../index.html"><span class="font-bold text-clip whitespace-nowrap">UnoAPI: updated v0.3.1</span>
</a>
<div class="relative overflow-hidden md:overflow-auto my-4 md:my-0 h-[calc(100vh-8rem)] md:h-auto">
<div class="overflow-y-auto h-full w-full relative pr-6"><nav class="table w-full min-w-full my-6 lg:my-8">
<p class="caption" role="heading"><span class="caption-text">Table of Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../00-preliminaries/preliminaries.html">About the Book</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introduction to Parallel Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10-software-engineering/software-engineering.html">Introduction to Software Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15-modern-cpp/modern-cpp.html">Modern C++ as a Better C (and C++)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../18-dpcpp/dpcpp.html">Data-Parallel C++ with oneAPI/SYCL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../30-performance/performance.html">Performance Essentials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../80-running/running.html">Compiling and Running oneAPI programs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../99-sandbox/issues.html">Issues</a></li>
</ul>
</nav>
</div>
</div>
<button @click="showSidebar = false" class="absolute md:hidden right-4 top-4 rounded-sm opacity-70 transition-opacity hover:opacity-100" type="button">
<svg class="h-4 w-4" fill="currentColor" height="24" stroke="none" viewbox="0 96 960 960" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M480 632 284 828q-11 11-28 11t-28-11q-11-11-11-28t11-28l196-196-196-196q-11-11-11-28t11-28q11-11 28-11t28 11l196 196 196-196q11-11 28-11t28 11q11 11 11 28t-11 28L536 576l196 196q11 11 11 28t-11 28q-11 11-28 11t-28-11L480 632Z"></path>
</svg>
</button>
</aside>
<main class="relative py-6 lg:gap-10 lg:py-8 xl:grid xl:grid-cols-[1fr_300px]">
<div class="w-full min-w-0 mx-auto">
<nav aria-label="breadcrumbs" class="flex items-center mb-4 space-x-1 text-sm text-muted-foreground">
<a class="overflow-hidden text-ellipsis whitespace-nowrap hover:text-foreground" href="../index.html">
<span class="hidden md:inline">UnoAPI: updated v0.3.1</span>
<svg aria-label="Home" class="md:hidden" fill="currentColor" height="18" stroke="none" viewbox="0 96 960 960" width="18" xmlns="http://www.w3.org/2000/svg">
<path d="M240 856h120V616h240v240h120V496L480 316 240 496v360Zm-80 80V456l320-240 320 240v480H520V696h-80v240H160Zm320-350Z"></path>
</svg>
</a>
<div class="mr-1">/</div><span aria-current="page" class="font-medium text-foreground overflow-hidden text-ellipsis whitespace-nowrap">Introduction to Parallel Programming</span>
</nav>
<div id="content" role="main">
<blockquote>
<div><p>single: Parallel Computing; Introduction
single: Parallel Programming; Concepts
single: Performance Evaluation; Parallel Computing
single: Computing Systems; History
single: Programming Techniques; Parallel</p>
</div></blockquote>
<section id="introduction-to-parallel-programming">
<h1>Introduction to Parallel Programming<a class="headerlink" href="#introduction-to-parallel-programming" title="Permalink to this heading">¶</a></h1>
<p>We introduce parallel computing.
We begin with a discussion of parallel computing history, including a discussion of computing systems and programming techniques.  We then cover key parallel programming concepts, specifically how to categorize parallel computing approaches and evaluate performance.
Knowing this foundational material remains of timeless importance when it comes to achieving desired results.</p>
<section id="notable-early-parallel-computing-systems">
<span id="index-0"></span><h2>Notable Early Parallel Computing Systems<a class="headerlink" href="#notable-early-parallel-computing-systems" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#notable-early-parallel-computing-systems'">¶</a></h2>
<p>The history of parallel computing systems dates nearly to the beginning of modern electronic computing history itself, which featured notable systems such as <a class="reference external" href="https://www.britannica.com/technology/Atanasoff-Berry-Computer">Atanasoff-Berry Computer (ABC)</a> (1939), <a class="reference external" href="https://www.britannica.com/technology/Colossus-computer">Colossus</a> (1943), and <a class="reference external" href="https://www.britannica.com/technology/ENIAC">ENIAC https://www.britannica.com/technology/ENIAC</a> (1945).</p>
<p>Here is a timeline of key developments in parallel computing, focusing on computers designed for parallel and scientific computing, with an emphasis on on-chip parallelism.</p>
<ul class="simple">
<li><p>In 1945, ENIAC, the first general-purpose electronic computer, laid the foundation for computing capabilities, though not specifically designed for parallelism. ILLIAC I (1952) and ILLIAC II (1957) were early examples of computers tailored for scientific and engineering calculations, with ILLIAC II introducing parallel computing capabilities.</p></li>
<li><p>In 1962, ILLIAC III emerged as a highly parallel computer, accommodating up to 256 processors for large-scale parallel computations. ILLIAC IV (1966) continued the trend with a highly parallel supercomputer featuring 64 independent processing nodes.</p></li>
<li><p>Convex C1 (1985) marked the advent of the first commercial parallel computer, utilizing custom-designed parallel processing architecture for scientific and engineering applications.</p></li>
<li><p>Cray X-MP (1985) employed vector processing and parallelism to achieve remarkable performance.</p></li>
<li><p>In 1991, the Thinking Machines CM-5 supercomputer utilized vector processors and a custom interconnect network to deliver high performance across diverse scientific and engineering applications.</p></li>
<li><p>The IBM SP (1993) featured a distributed-memory architecture and high-speed interconnect network for impressive scientific computing performance. Beyond 1993, parallel computing became more ubiquitous, evolving into a commodity. Clusters, accelerators, and other specialized parallel hardware emerged, which we cover separately in the subsequent section.</p></li>
</ul>
<section id="cluster-computing">
<span id="index-1"></span><h3>Cluster Computing<a class="headerlink" href="#cluster-computing" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#cluster-computing'">¶</a></h3>
<p>Cluster computing, also known as high-performance computing (HPC), has its origins in the early days of computing when it became clear that solving complex computational problems required more processing power than a single machine could provide.
The concept of connecting multiple computers together to work in parallel emerged as a solution and is the basis for modern supercomputers such as those found on the <a class="reference external" href="https://www.top500.org/">Top 500 List</a>.</p>
<ul class="simple">
<li><p>In the 1960s and 1970s, research institutions and government agencies began experimenting with cluster-like architectures, such as the Control Data Corporation’s CDC 6600 system, which employed multiple processors to tackle scientific calculations.</p></li>
<li><p>In the 1990s, cluster computing gained widespread recognition with the advent of inexpensive, commodity hardware, and the development of networking technologies that allowed computers to communicate efficiently.</p></li>
<li><p>The Beowulf project, initiated by NASA in the mid-1990s, played a crucial role in popularizing cluster computing by demonstrating that a cluster of interconnected off-the-shelf computers could deliver impressive computing power at a fraction of the cost of traditional supercomputers.</p></li>
</ul>
<p>Since then, cluster computing has evolved significantly, becoming a dominant paradigm for high-performance computing, enabling breakthroughs in fields such as weather modeling, drug discovery, and data analysis.</p>
</section>
<section id="accelerators">
<span id="index-2"></span><h3>Accelerators<a class="headerlink" href="#accelerators" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#accelerators'">¶</a></h3>
<p>Accelerators and co-processors play a crucial role in enhancing the computational capabilities of central processing units (CPUs). These specialized hardware components are designed to offload specific tasks or types of computations from the CPU, thereby accelerating the overall performance of a system. By leveraging parallel processing techniques and tailored architectures, accelerators such as graphics processing units (GPUs), field-programmable gate arrays (FPGAs), and application-specific integrated circuits (ASICs) excel at performing repetitive and data-intensive operations.</p>
<p>GPUs, originally developed for rendering graphics, have found widespread use in areas such as scientific simulations, machine learning, and cryptocurrency mining. FPGAs offer reconfigurability and low power consumption, making them suitable for prototyping and custom logic implementation. ASICs, on the other hand, are highly optimized for specific applications and deliver exceptional performance and efficiency. Incorporating accelerators and co-processors alongside CPUs enables a heterogeneous computing environment, allowing for more efficient and specialized processing tailored to the demands of modern workloads.</p>
<p>Among the most common accelerators are the following:</p>
<ol class="arabic simple">
<li><p>Graphics Processing Units (GPUs): Originally designed for rendering graphics, GPUs have become immensely popular for their parallel computing capabilities. They excel in tasks such as scientific computing, deep learning, data analytics, and image processing.</p></li>
<li><p>Tensor Processing Units (TPUs): Developed by Google, TPUs are specialized hardware accelerators designed specifically for machine learning workloads. They offer high-speed matrix operations and optimized support for neural networks, making them ideal for AI-related tasks.</p></li>
<li><p>Field-Programmable Gate Arrays (FPGAs): FPGAs provide customizable hardware acceleration, allowing users to design and implement custom logic circuits to accelerate specific workloads. They are used in various domains, including high-frequency trading, network processing, and real-time data analysis.</p></li>
<li><p>Application-Specific Integrated Circuits (ASICs): ASICs are custom-built chips tailored for specific applications. They deliver exceptional performance and power efficiency by optimizing hardware for a particular task. Examples include Bitmain’s ASICs for cryptocurrency mining and Google’s custom ASICs for artificial intelligence.</p></li>
<li><p>Intel Xeon Phi: The Intel Xeon Phi, based on the Many Integrated Core (MIC) architecture, is designed for highly parallel workloads. It provides a large number of cores and high memory bandwidth, making it suitable for scientific simulations, molecular dynamics, and other computationally intensive tasks.</p></li>
<li><p>Dataflow Processing Units (DPUs): DPUs are emerging accelerators focused on accelerating data-intensive workloads such as networking, security, and storage. They offload tasks related to packet processing, encryption, and compression, improving overall system performance and efficiency.</p></li>
</ol>
<p>It’s worth noting that the field of accelerators is rapidly evolving, and new innovations and technologies continue to emerge, catering to specific application domains and computing needs.</p>
<p>The first historical examples of accelerators can be traced back to the early days of computing when specialized hardware components were developed to enhance the performance of computer systems. Here are a few notable examples:</p>
<ol class="arabic simple">
<li><p>Floating-Point Units (FPUs): In the 1970s, separate floating-point units were introduced as co-processors to CPUs. These dedicated units were designed to handle floating-point arithmetic operations more efficiently, improving the computational capabilities of the system. They played a significant role in scientific and engineering applications that required extensive floating-point calculations.</p></li>
<li><p>Math Co-processors: In the 1980s, math co-processors emerged as additional chips that could be added to the CPU to offload mathematical computations. Intel’s 8087 math co-processor, for example, provided hardware acceleration for floating-point arithmetic, enabling faster and more precise mathematical calculations on compatible systems.</p></li>
<li><p>Graphics Co-processors: The development of graphics co-processors in the 1980s marked a milestone in accelerating graphical computations. Companies like IBM and Texas Instruments introduced graphics co-processors that worked alongside CPUs to offload the processing required for displaying graphics, resulting in smoother and faster graphical user interfaces.</p></li>
<li><p>Sound Blaster Cards: Sound Blaster cards, introduced by Creative Labs in the late 1980s and early 1990s, acted as accelerators for audio processing in personal computers. They provided dedicated hardware for sound synthesis, audio playback, and enhanced sound effects, freeing up the CPU from intensive audio processing tasks.</p></li>
</ol>
<p>These early accelerators paved the way for the development of more specialized and powerful hardware accelerators that emerged later, such as GPUs, FPGAs, and ASICs, to address the growing demands of specific computational tasks and domains.</p>
</section>
<section id="vector-machines-cray">
<span id="index-3"></span><h3>Vector machines (Cray)<a class="headerlink" href="#vector-machines-cray" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#vector-machines-cray'">¶</a></h3>
<p>Cray, a renowned supercomputer company founded by Seymour Cray, played a pivotal role in advancing the field of vector processing. Vector processing is a technique that allows a single instruction to operate on multiple data elements simultaneously, enabling efficient execution of computationally intensive tasks. Cray supercomputers, starting with the Cray-1 in the 1970s, were among the first to incorporate vector processing architecture. By leveraging specialized vector registers and instructions, Cray systems achieved remarkable performance gains in scientific simulations, weather forecasting, and other applications that required large-scale numerical computations. The Cray-1, with its distinctive “C” shape design and liquid cooling system, became an iconic symbol of supercomputing prowess. Subsequent generations of Cray systems, including the Cray-2, Cray X-MP, and Cray Y-MP, continued to refine and enhance vector processing capabilities, pushing the boundaries of computational performance. Even though modern supercomputers have evolved beyond pure vector processing, Cray’s contributions to this field were instrumental in establishing the foundation for high-performance computing and shaping the development of future supercomputer architectures.</p>
</section>
<section id="connection-machine-thinking-machines">
<span id="index-4"></span><h3>Connection Machine (Thinking Machines)<a class="headerlink" href="#connection-machine-thinking-machines" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#connection-machine-thinking-machines'">¶</a></h3>
<p>The Connection Machine, developed by Danny Hillis and his team at Thinking Machines Corporation in the 1980s, was a highly influential supercomputer known for its parallel processing capabilities. The name “Connection Machine” derived from its unique architectural design, inspired by the concept of massively parallel processing. Instead of relying on a small number of powerful processors, the Connection Machine employed a large number of simpler processors called “nodes” that communicated and coordinated their activities through a high-speed network. Each node had its own local memory, and computations were performed in parallel across multiple nodes, enabling the machine to tackle complex problems through distributed processing. The Connection Machine gained attention for its ability to handle massive amounts of data and execute tasks in parallel, making it suitable for applications like artificial intelligence, pattern recognition, and scientific simulations. This innovative approach to parallel computing made the Connection Machine a groundbreaking contribution to the field of supercomputing.</p>
</section>
<section id="systolic-architectures">
<span id="index-5"></span><h3>Systolic architectures<a class="headerlink" href="#systolic-architectures" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#systolic-architectures'">¶</a></h3>
<p>Systolic array architectures are a type of parallel computing design inspired by the human heart’s rhythmic pumping action. In systolic arrays, data flows through a network of specialized processing elements arranged in a regular grid-like pattern. Each processing element performs a simple computation and passes the results to its neighboring elements. This design facilitates the efficient execution of iterative computations and allows for a high degree of parallelism. However, despite their promising potential, systolic array architectures did not experience widespread adoption. Several factors contributed to this. Firstly, systolic arrays require highly regular data access patterns, making them less suitable for irregular or unpredictable computations. Secondly, the complexity of designing and programming systolic arrays posed significant challenges for developers. Lastly, the advent of other parallel computing architectures, such as SIMD (Single Instruction, Multiple Data) and MIMD (Multiple Instruction, Multiple Data), provided more flexibility and better suited the diverse range of applications. However, it’s worth noting that the spirit of systolic arrays lives on in various forms. Modern incarnations include specialized hardware accelerators, such as TPUs and FPGAs, which employ array-like structures and dataflow architectures to achieve high-performance computing for specific workloads. Additionally, some parallel programming frameworks and languages incorporate systolic-like concepts to optimize data movement and parallel execution in distributed systems.</p>
</section>
<section id="loop-parallelism-in-fortran">
<span id="index-6"></span><h3>Loop parallelism in FORTRAN<a class="headerlink" href="#loop-parallelism-in-fortran" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#loop-parallelism-in-fortran'">¶</a></h3>
<p>Loop parallelism techniques, including the use of compiler directives like OpenMP, began to appear in the late 1990s and early 2000s. OpenMP, for example, was first introduced in 1997 as an application programming interface (API) specification for shared-memory parallel programming. The initial versions of OpenMP were primarily targeted at Fortran and C/C++ languages. Over the years, OpenMP has evolved and gained wider adoption, becoming a popular choice for parallel programming in scientific and high-performance computing domains. However, it’s important to note that specific implementations and support for loop parallelism may have varied among different compilers and versions during that time.</p>
<p>In early versions of Fortran, loop parallelism was typically achieved through manual techniques and compiler directives. Here’s an example that demonstrates the computation of the area under a curve and the vector dot product using loop parallelism in Fortran:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These examples can be found in the <a class="reference external" href="https://github.com/LoyolaChicagoCode/unoapi-fortran-exmaples">LoyolaChicagoCode/unoapi-fortran-exmaples</a> repository. These examples exist to show the relative merts of using OpenMPI/SYCL vs. lower level methods. This book is not about FORTRAN; however, this history will help to understand the sigificance of  modern C++ efforts.`</p>
</div>
<div class="highlight-fortran notranslate"><div class="highlight"><pre><span></span><span class="c">! Computation of the area under a curve using loop parallelism</span>
<span class="k">program </span><span class="n">AreaUnderCurve</span>
<span class="w">  </span><span class="k">implicit none</span>
<span class="k">  </span><span class="kt">integer</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">n</span>
<span class="w">  </span><span class="kt">real</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">dx</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">area</span>

<span class="w">  </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1000</span><span class="w">  </span><span class="c">! Number of intervals</span>
<span class="w">  </span><span class="n">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="w">   </span><span class="c">! Lower limit of integration</span>
<span class="w">  </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span><span class="w">   </span><span class="c">! Upper limit of integration</span>
<span class="w">  </span><span class="n">dx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">b</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">a</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">n</span><span class="w">  </span><span class="c">! Width of each interval</span>

<span class="w">  </span><span class="n">area</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span>

<span class="w">  </span><span class="c">!$OMP PARALLEL DO DEFAULT(NONE) PRIVATE(x) SHARED(a, dx, n, area)</span>
<span class="w">  </span><span class="k">do </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">n</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">dx</span><span class="w">  </span><span class="c">! Midpoint of the interval</span>
<span class="w">    </span><span class="n">area</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">area</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">dx</span>
<span class="w">  </span><span class="k">end do</span>
<span class="w">  </span><span class="c">!$OMP END PARALLEL DO</span>

<span class="w">  </span><span class="k">print</span><span class="w"> </span><span class="o">*</span><span class="p">,</span><span class="w"> </span><span class="s2">"The area under the curve is:"</span><span class="p">,</span><span class="w"> </span><span class="n">area</span>

<span class="k">contains</span>

<span class="w">  </span><span class="c">! Function representing the curve</span>
<span class="w">  </span><span class="k">function </span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">    </span><span class="kt">real</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">x</span>
<span class="w">    </span><span class="kt">real</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">f</span>

<span class="w">    </span><span class="n">f</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="w">  </span><span class="c">! Example function: x^2</span>
<span class="w">  </span><span class="k">end function </span><span class="n">f</span>

<span class="k">end program </span><span class="n">AreaUnderCurve</span>
</pre></div>
</div>
<p>This is another example of how to compute the vector dot product in F90.</p>
<div class="highlight-fortran notranslate"><div class="highlight"><pre><span></span><span class="c">! Computation of the dot product of two vectors using loop parallelism</span>
<span class="k">program </span><span class="n">VectorDotProduct</span>
<span class="w">  </span><span class="k">implicit none</span>
<span class="k">  </span><span class="kt">integer</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">n</span>
<span class="w">  </span><span class="kt">real</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="nb">dot_product</span>
<span class="nb">  </span><span class="kt">real</span><span class="p">,</span><span class="w"> </span><span class="k">dimension</span><span class="p">(:),</span><span class="w"> </span><span class="k">allocatable</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">vector1</span><span class="p">,</span><span class="w"> </span><span class="n">vector2</span>

<span class="w">  </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">10000</span><span class="w">  </span><span class="c">! Number of elements in the vectors</span>
<span class="w">  </span><span class="k">allocate</span><span class="p">(</span><span class="n">vector1</span><span class="p">(</span><span class="n">n</span><span class="p">),</span><span class="w"> </span><span class="n">vector2</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>

<span class="w">  </span><span class="c">! Initialize the vectors (example values)</span>
<span class="w">  </span><span class="n">vector1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span>
<span class="w">  </span><span class="n">vector2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">2.0</span>

<span class="w">  </span><span class="nb">dot_product</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span>

<span class="w">  </span><span class="c">!$OMP PARALLEL DO DEFAULT(NONE) PRIVATE(i) SHARED(n, vector1, vector2, dot_product)</span>
<span class="w">  </span><span class="k">do </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">n</span>
<span class="w">    </span><span class="nb">dot_product</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">dot_product</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">vector1</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">vector2</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<span class="w">  </span><span class="k">end do</span>
<span class="w">  </span><span class="c">!$OMP END PARALLEL DO</span>

<span class="w">  </span><span class="k">print</span><span class="w"> </span><span class="o">*</span><span class="p">,</span><span class="w"> </span><span class="s2">"The dot product is:"</span><span class="p">,</span><span class="w"> </span><span class="nb">dot_product</span>

<span class="k">end program </span><span class="n">VectorDotProduct</span>
</pre></div>
</div>
<p>Similar to our C++ code, you can create a <code class="docutils literal notranslate"><span class="pre">CMakeLists.txt1</span></code> for building FORTRAN programs:</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">cmake_minimum_required</span><span class="p">(</span><span class="s">VERSION</span><span class="w"> </span><span class="s">3.10</span><span class="p">)</span>

<span class="nb">project</span><span class="p">(</span><span class="s">Fortran-OpenMP-Examples</span><span class="p">)</span>

<span class="c"># Enable Fortran</span>
<span class="nb">enable_language</span><span class="p">(</span><span class="s">Fortran</span><span class="p">)</span>
<span class="w">     </span>
<span class="c"># Bin directory</span>
<span class="nb">set</span><span class="p">(</span><span class="s">CMAKE_RUNTIME_OUTPUT_DIRECTORY</span><span class="w"> </span><span class="o">${</span><span class="nv">CMAKE_BINARY_DIR</span><span class="o">}</span><span class="s">/bin</span><span class="p">)</span>

<span class="c"># Find OpenMP</span>
<span class="nb">find_package</span><span class="p">(</span><span class="s">OpenMP</span><span class="w"> </span><span class="s">REQUIRED</span><span class="p">)</span>

<span class="c"># Add the executable</span>
<span class="nb">add_executable</span><span class="p">(</span><span class="s">aoc</span><span class="w"> </span><span class="s">aoc.f90</span><span class="p">)</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">aoc</span><span class="w"> </span><span class="s">PRIVATE</span><span class="w"> </span><span class="s">OpenMP::OpenMP_Fortran</span><span class="p">)</span>

<span class="nb">add_executable</span><span class="p">(</span><span class="s">vdp</span><span class="w"> </span><span class="s">vdp.f90</span><span class="p">)</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">vdp</span><span class="w"> </span><span class="s">PRIVATE</span><span class="w"> </span><span class="s">OpenMP::OpenMP_Fortran</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="c-and-data-parallel-c-efforts">
<h3>C* and Data-Parallel C efforts<a class="headerlink" href="#c-and-data-parallel-c-efforts" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#c-and-data-parallel-c-efforts'">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We’re still working on this section.</p>
</div>
<ul class="simple">
<li><p>Language Spec: <a class="reference external" href="http://people.csail.mit.edu/bradley/cm5docs/CStarProgrammingGuide.pdf">http://people.csail.mit.edu/bradley/cm5docs/CStarProgrammingGuide.pdf</a></p></li>
<li><p>Data-Parallel on MIMD Computers: P. J. Hatcher, M. J. Quinn, A. J. Lapadula, B. K. Seevers, R. J. Anderson and R. R. Jones, “Data-parallel programming on MIMD computers,” in IEEE Transactions on Parallel and Distributed Systems, vol. 2, no. 3, pp. 377-383, July 1991, doi: 10.1109/71.86112, <a class="reference external" href="https://ieeexplore.ieee.org/document/86112">https://ieeexplore.ieee.org/document/86112</a></p></li>
</ul>
<span class="target" id="cuda"></span></section>
<section id="index-7">
<span id="id1"></span><h3>CUDA<a class="headerlink" href="#index-7" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#index-7'">¶</a></h3>
<p>CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA. It allows developers to harness the power of NVIDIA GPUs (Graphics Processing Units) to accelerate computationally intensive tasks. By utilizing CUDA, developers can write code that offloads parallel computations to the GPU, enabling significant speedups and performance improvements in a wide range of applications, such as scientific simulations, data analysis, machine learning, and more.</p>
<p id="index-8">CUDA became popular due to several key factors. First, NVIDIA GPUs had already gained a significant market share in graphics rendering, providing a large user base to leverage for parallel computing. Second, CUDA offered substantial performance acceleration by harnessing the massive parallel architecture of GPUs, allowing developers to offload computationally intensive tasks and achieve significant speedups. Third, CUDA provided a developer-friendly programming model, extending the C language with directives and APIs that made it easier to express parallelism and utilize GPU resources. Fourth, NVIDIA’s support ecosystem was comprehensive, offering tools, libraries, and documentation to aid CUDA development. The diverse range of application domains, including scientific simulations, data analytics, machine learning, and image processing, further contributed to CUDA’s popularity. Lastly, the availability of NVIDIA’s powerful GPU hardware across various price points enabled wider accessibility and adoption of CUDA in different industries and research fields.</p>
<p>Here’s the CUDA code example for computing the vector dot product in reStructuredText format:</p>
<div class="highlight-cuda notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="cp">#define SIZE 10000</span>

<span class="c1">// CUDA kernel for dot product computation</span>
<span class="kr">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">dotProduct</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">vector1</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">vector2</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">result</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">tid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nb">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nb">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tid</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">result</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">vector1</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">vector2</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="n">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">vector1</span><span class="p">[</span><span class="n">SIZE</span><span class="p">];</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">vector2</span><span class="p">[</span><span class="n">SIZE</span><span class="p">];</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">result</span><span class="p">[</span><span class="n">SIZE</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// Initialize the vectors (example values)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">SIZE</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">vector1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span><span class="p">;</span>
<span class="w">        </span><span class="n">vector2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">2.0</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">d_vector1</span><span class="p">;</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">d_vector2</span><span class="p">;</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">d_result</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Allocate device memory</span>
<span class="w">    </span><span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_vector1</span><span class="p">,</span><span class="w"> </span><span class="n">SIZE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
<span class="w">    </span><span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_vector2</span><span class="p">,</span><span class="w"> </span><span class="n">SIZE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
<span class="w">    </span><span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_result</span><span class="p">,</span><span class="w"> </span><span class="n">SIZE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>

<span class="w">    </span><span class="c1">// Copy input vectors from host to device</span>
<span class="w">    </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_vector1</span><span class="p">,</span><span class="w"> </span><span class="n">vector1</span><span class="p">,</span><span class="w"> </span><span class="n">SIZE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span><span class="w"> </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_vector2</span><span class="p">,</span><span class="w"> </span><span class="n">vector2</span><span class="p">,</span><span class="w"> </span><span class="n">SIZE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span><span class="w"> </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Launch the dot product kernel</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">blockSize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">256</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">gridSize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">SIZE</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">blockSize</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">blockSize</span><span class="p">;</span>
<span class="w">    </span><span class="n">dotProduct</span><span class="o">&lt;&lt;&lt;</span><span class="n">gridSize</span><span class="p">,</span><span class="w"> </span><span class="n">blockSize</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_vector1</span><span class="p">,</span><span class="w"> </span><span class="n">d_vector2</span><span class="p">,</span><span class="w"> </span><span class="n">d_result</span><span class="p">,</span><span class="w"> </span><span class="n">SIZE</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Copy result from device to host</span>
<span class="w">    </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="w"> </span><span class="n">d_result</span><span class="p">,</span><span class="w"> </span><span class="n">SIZE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span><span class="w"> </span><span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Compute the dot product result on the host</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">dotProductResult</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">SIZE</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">dotProductResult</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">"The dot product is: %f</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="w"> </span><span class="n">dotProductResult</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Free device memory</span>
<span class="w">    </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">d_vector1</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">d_vector2</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">d_result</span><span class="p">);</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p id="index-9">The above CUDA example demonstrates the computation of the dot product of two vectors using GPU parallelism. The code begins by initializing the input vectors with example values on the host. Then, device memory is allocated using <code class="docutils literal notranslate"><span class="pre">cudaMalloc</span></code>, and the input vectors are copied from the host to the device memory using <code class="docutils literal notranslate"><span class="pre">cudaMemcpy</span></code>. The dot product computation is performed by launching a CUDA kernel, <code class="docutils literal notranslate"><span class="pre">dotProduct</span></code>, which runs in parallel on the GPU. Each thread calculates the product of corresponding elements from the input vectors. After the kernel execution, the results are copied back to the host memory using <code class="docutils literal notranslate"><span class="pre">cudaMemcpy</span></code>, and the dot product is computed on the host. Finally, the dot product result is printed to the console. The example showcases how CUDA leverages the parallel processing capabilities of GPUs to accelerate computations, leading to improved performance compared to traditional CPU-based implementations.</p>
</section>
<section id="openmp">
<h3>OpenMP<a class="headerlink" href="#openmp" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#openmp'">¶</a></h3>
<p>OpenMP (Open Multi-Processing) is a portable API (Application Programming Interface) for shared-memory parallel programming in C, C++, and Fortran. It was developed in the late 1990s by various hardware and software vendors, aiming to create a standardized and user-friendly programming model for shared-memory parallelism. The initial release, OpenMP 1.0, came out in 1997, offering basic parallelism constructs. Subsequent versions, including OpenMP 2.0 (1998) and OpenMP 3.0 (2008), introduced additional features such as task parallelism and nested parallelism. OpenMP gained widespread adoption in high-performance computing due to its simplicity, portability, and compatibility with existing codebases. OpenMP 5.0, released in 2018, brought advanced features like improved accelerator offloading, enhanced tasking capabilities, and SIMD programming support. It remains actively developed and maintained by a consortium of vendors, researchers, and users. OpenMP enables developers to leverage multi-core processors and parallel architectures for enhanced performance and scalability in their applications.</p>
<p>C, C++, and Fortran, are the primary languages directly supported by OpenMP. OpenMP offers language extensions, compiler directives, and runtime libraries specific to each language to facilitate parallel programming and exploit shared-memory parallelism:</p>
<ol class="arabic simple">
<li><p>C: OpenMP directives and runtime library routines can be used directly within C programs to express parallelism and control the execution of parallel regions.</p></li>
<li><p>C++: OpenMP can be utilized within C++ programs, providing directives and library routines to enable shared-memory parallel programming.</p></li>
<li><p>Fortran: OpenMP is natively supported in Fortran, allowing Fortran programmers to express parallelism through directives and runtime routines for efficient parallel execution.</p></li>
</ol>
<p>Here is an example of an OpenMP program that computes the dot product of two vectors.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdlib.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>

<span class="cp">#define SIZE 10000</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="kt">float</span><span class="w"> </span><span class="n">vector1</span><span class="p">[</span><span class="n">SIZE</span><span class="p">];</span>
<span class="w">   </span><span class="kt">float</span><span class="w"> </span><span class="n">vector2</span><span class="p">[</span><span class="n">SIZE</span><span class="p">];</span>
<span class="w">   </span><span class="kt">float</span><span class="w"> </span><span class="n">dotProduct</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>
<span class="w">   </span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="p">;</span>

<span class="w">   </span><span class="c1">// Initialize the vectors with random numbers</span>
<span class="w">   </span><span class="cp">#pragma omp parallel for</span>
<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">SIZE</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">vector1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">rand</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">RAND_MAX</span><span class="p">;</span>
<span class="w">      </span><span class="n">vector2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">rand</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">RAND_MAX</span><span class="p">;</span>
<span class="w">   </span><span class="p">}</span>

<span class="w">   </span><span class="c1">// Compute the dot product using OpenMP parallelism</span>
<span class="w">   </span><span class="cp">#pragma omp parallel for reduction(+:dotProduct)</span>
<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">SIZE</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">dotProduct</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">vector1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">vector2</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">   </span><span class="p">}</span>

<span class="w">   </span><span class="n">printf</span><span class="p">(</span><span class="s">"The dot product is: %f</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="w"> </span><span class="n">dotProduct</span><span class="p">);</span>

<span class="w">   </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Here is how it would look in C++ with <code class="docutils literal notranslate"><span class="pre">std::vector</span></code>:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;vector&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cstdlib&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;omp.h&gt;</span>

<span class="cp">#define SIZE 10000</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">vector1</span><span class="p">(</span><span class="n">SIZE</span><span class="p">);</span>
<span class="w">   </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">vector2</span><span class="p">(</span><span class="n">SIZE</span><span class="p">);</span>
<span class="w">   </span><span class="kt">float</span><span class="w"> </span><span class="n">dotProduct</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>

<span class="w">   </span><span class="c1">// Initialize the vectors with random numbers</span>
<span class="w">   </span><span class="cp">#pragma omp parallel for</span>
<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">SIZE</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">vector1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">rand</span><span class="p">())</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">RAND_MAX</span><span class="p">;</span>
<span class="w">      </span><span class="n">vector2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">rand</span><span class="p">())</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">RAND_MAX</span><span class="p">;</span>
<span class="w">   </span><span class="p">}</span>

<span class="w">   </span><span class="c1">// Compute the dot product using OpenMP parallelism</span>
<span class="w">   </span><span class="cp">#pragma omp parallel for reduction(+:dotProduct)</span>
<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">SIZE</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">dotProduct</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">vector1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">vector2</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">   </span><span class="p">}</span>

<span class="w">   </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">"The dot product is: "</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">dotProduct</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="w">   </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>And here is a FORTRAN version:</p>
<div class="highlight-fortran notranslate"><div class="highlight"><pre><span></span><span class="k">program </span><span class="nb">dot_product</span>
<span class="nb">   </span><span class="k">use </span><span class="n">omp_lib</span>
<span class="w">   </span><span class="k">implicit none</span>

<span class="k">   </span><span class="kt">integer</span><span class="p">,</span><span class="w"> </span><span class="k">parameter</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">SIZE</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">10000</span>
<span class="w">   </span><span class="kt">real</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">vector1</span><span class="p">(</span><span class="n">SIZE</span><span class="p">),</span><span class="w"> </span><span class="n">vector2</span><span class="p">(</span><span class="n">SIZE</span><span class="p">),</span><span class="w"> </span><span class="n">dotProduct</span>
<span class="w">   </span><span class="kt">integer</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">i</span>

<span class="w">   </span><span class="c">! Initialize the vectors with random numbers</span>
<span class="w">   </span><span class="c">!$OMP PARALLEL DO DEFAULT(NONE) PRIVATE(i) SHARED(vector1, vector2)</span>
<span class="w">   </span><span class="k">do </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">SIZE</span>
<span class="w">      </span><span class="n">vector1</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kt">real</span><span class="p">(</span><span class="nb">rand</span><span class="p">())</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="kt">real</span><span class="p">(</span><span class="nb">RAND</span><span class="p">())</span>
<span class="w">      </span><span class="n">vector2</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kt">real</span><span class="p">(</span><span class="nb">rand</span><span class="p">())</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="kt">real</span><span class="p">(</span><span class="nb">RAND</span><span class="p">())</span>
<span class="w">   </span><span class="k">end do</span>
<span class="w">   </span><span class="c">!$OMP END PARALLEL DO</span>

<span class="w">   </span><span class="n">dotProduct</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span>

<span class="w">   </span><span class="c">! Compute the dot product using OpenMP parallelism</span>
<span class="w">   </span><span class="c">!$OMP PARALLEL DO DEFAULT(NONE) PRIVATE(i) SHARED(vector1, vector2) REDUCTION(+:dotProduct)</span>
<span class="w">   </span><span class="k">do </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">SIZE</span>
<span class="w">      </span><span class="n">dotProduct</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dotProduct</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">vector1</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">vector2</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<span class="w">   </span><span class="k">end do</span>
<span class="w">   </span><span class="c">!$OMP END PARALLEL DO</span>

<span class="w">   </span><span class="k">write</span><span class="p">(</span><span class="o">*</span><span class="p">,</span><span class="w"> </span><span class="s2">"(A, F6.2)"</span><span class="p">)</span><span class="w"> </span><span class="s2">"The dot product is:"</span><span class="p">,</span><span class="w"> </span><span class="n">dotProduct</span>

<span class="k">end program </span><span class="nb">dot_product</span>
</pre></div>
</div>
<p>As you can see, a strength of OpenMP is the ability to write similar code to express parallelism in all three of the languages.
This legacy is important to frameworks such as oneAPI/SYCL, which aim to provide a similar framework without relying on compiler pramgas.
In particular, we will discuss oneAPI in more detail <a class="reference internal" href="../18-dpcpp/dpcpp.html"><span class="doc">in the section on data-parallel C++</span></a> below.</p>
</section>
</section>
<section id="parallel-concepts-for-humans">
<h2>Parallel Concepts for Humans<a class="headerlink" href="#parallel-concepts-for-humans" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#parallel-concepts-for-humans'">¶</a></h2>
<p>In this section, we’ll discuss some of the key concepts and principles underlying parallel computing.
The track for this material was laid in the book <a class="reference external" href="https://ecommons.luc.edu/cs_facpubs/3/">High Performance Java Platform Computing</a>.</p>
<p><a class="reference external" href="https://ecommons.luc.edu/cs_facpubs/3/">https://ecommons.luc.edu/cs_facpubs/3/</a></p>
<p><a class="reference external" href="https://figshare.com/articles/dataset/hpjpc/962958">https://figshare.com/articles/dataset/hpjpc/962958</a></p>
<section id="von-neumann-machines-and-their-limits">
<h3>von Neumann machines and their limits<a class="headerlink" href="#von-neumann-machines-and-their-limits" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#von-neumann-machines-and-their-limits'">¶</a></h3>
<p>A discussion of parallel computing must begin with a discussion of sequential computing and the von Neumann machine—our sequential computer. The von Neumann machine is one of the computer designs of John von Neumann. A processor fetches instructions and data from a memory, operates on the data, and writes the results back into memory. Computation is accomplished by making small incremental changes in the global memory.</p>
<p>The problem with the von Neumann machine is that the design relies on making a sequence of small changes, a highly sequential process. Note that current programming languages are designed assuming that the von Neumann machine will be used. The assignment statement fetches data from memory on the right hand side, performs computations, and writes results back into the memory for the left-hand-side variable. The statements are executed sequentially, with control accomplished by branches. In the language, the branches are given the syntactic sugar of if statements, while statements, and so on.</p>
<p>There is a problem in trying to speed up von Neumann machines. They are inherently sequential in principle. Attempts may be made to execute several instructions at once (super-scalar execution), but that gives only a few times the speedup. Similarly, it is difficult to gain high speedup from a program written in a von Neumann language without doing an extensive rewrite of the program.</p>
</section>
<section id="flynn-s-taxonomy">
<h3>Flynn’s taxonomy<a class="headerlink" href="#flynn-s-taxonomy" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#flynn-s-taxonomy'">¶</a></h3>
<p>Flynn produced a taxonomy of parallel machines that is widely used. He classified computers with respect to how many different instruction streams they are fetching and executing at the same time, and by how many data sets (data streams) they are fetching and processing. His taxonomy is as follows:</p>
<ul>
<li><p>SISD—single instruction stream–single data stream: the familiar von Neumann machine. Fetching one sequence of instructions and fetching the data and the instructions address from one memory.</p></li>
<li><p>MIMD—(pronounced “mim-dee”) multiple instruction–multiple data stream: a multiprocessor or multicomputer (and the subject of this book). Here, several processors are fetching their own instructions and operating on the data those instructions specify. To gain speedup on individual programs, these processors must synchronize and communicate with each other.</p></li>
<li><p>SIMD—(pronounced “sim-dee”) single instruction stream–multiple data stream: These machines typically are used to process arrays. A single processor fetches instructions and broadcasts those instructions to a number of data units. Those data units fetch data and perform operations on them. The appropriate programming language for such machines has a single flow of control (like a Von Neumann language), but has operations that can operate on entire arrays, rather than on individual array elements. The hardware needs ways in which data units will not execute some operations based on tests of their own data (e.g., so that some units can turn off for the then and others for the else parts of if-then-else statements), and it needs to let the control unit read the and or the or of the results of data tests at the data units (e.g. to know when all units have finished executing a while loop).</p></li>
<li><p>MISD—multiple instruction stream–single data stream: It’s not totally clear what machines fit into this category. One kind of MISD machine would be designed for fail-safe operation; several processors perform the same operations on the same data and check each other to be sure that any failure will be caught. (An early system that worked in this manner was the <a class="reference external" href="https://en.wikipedia.org/wiki/Tandem_Computers">Tandem</a>).</p>
<p>Another proposed MISD machine is a systolic array processor. Streams of data are fetched from memory and passed through arrays of processors. The individual processors perform their own operations on the streams of data passing through them, but they have no control over where to fetch data from.</p>
</li>
</ul>
<p>MIMD machines are divided into two varieties: shared memory and distributed memory.</p>
<p>Shared-memory machines have several processors accessing a common memory. Unless the machine is for a special purpose, the processors will be accessing the shared memory through some sort of address-mapping hardware. To be used for parallel processing, the software must let the processors actually share that memory in their address spaces.</p>
<p>Shared-memory machines have significant advantages for programming. All of the processors working on a common problem can share the large data structures (e.g., large arrays) and cooperate by working on parts of the data structure, while other processors work on other parts.</p>
<p>The problems with programming shared-memory machines have to do with synchronizing the processors. Since the processors work by fetching and storing small data elements, a processor updating a large data structure cannot read or write it in one instant. This means that if one processor is reading and another writing the same data structure at the same time, the reader may be getting some old components and some new ones. The state will not be consistent, and the computation will therefore become confused (“confused,” meaning there is an inconsistent state.) Similarly, if two processors are trying to write the same structure at the same time, parts of the two writes will be confused. Therefore, the software for shared-memory parallel machines must provide facilities for coordinating processors. The problem with programming is to make sure the coordination is done correctly.</p>
<p>There is another problem with shared-memory machines: It’s hard to build them to be large. The switch between the processors and memory becomes a bottleneck, limiting the traffic between processors and memory, or it tends to become expensive or slow. This is particularly the problem with UMA machines (Uniform Memory Access). UMA machines take the same amount of time to get to all memory locations. As the UMA machines get larger, physical packaging alone dictates that some memory will get further from some processors than it will for smaller versions. When the problems of switching more processors to more memory chips are added, UMAs can be expected to get slower still.</p>
<p>An alternative to UMA is NUMA (nonuniform memory access) machines. Typically, NUMA machines have some memory attached to each processor, which the processor can access quickly. To get to the memory attached to another processor, the processor must go through some sort of switch, which slows down the access. By careful placement and rep-lication of some data and subroutines, NUMA machines can have many of the programming conveniences of UMA machines, but with cheaper hardware, larger numbers of processors, and reasonable speed. However, programmers tend to discover that they can gain even better performance by copying entire data structures into local memory, operating on them locally, and writing them back to local memory. At this point, their code becomes more complex and less portable.Distributed-memory MIMD machines (MIMD-DM) are much easier to build, but much harder to program. The MIMD-DM machine is basically a collection of computers, called nodes, connected through a high-performance network. The major programming problem is that the individual machines must coordinate and communicate data by message passing; it often requires entire redesigns of programs to port them to MIMD-DM machines. The only way to access data on another node is to have that node send it to you. For that to happen, the other node must know that you need the data—or you must send it a request message, and it must be programmed to reply.</p>
<p>It is arguable that the change from shared-memory to distributed-memory machines is a radical shift (but one becoming more common); it goes to the root of how one thinks about computations. On the von Neumann machine, the most important entity is the process, the program in execution. The process fetches instructions and manipulates data, thereby embodying both control and data. On a shared-memory machine, the process is still the most important entity, although there are now multiple threads of execution within the data space. But at the global level, on a distributed-memory machine, messages convey data across the machine and the arrival of messages enables computations. At the global level of distributed-memory machines, it is the messages that embody control and data. Hence, the messages are more important than the processes that are running on the nodes.</p>
</section>
<section id="control-memory-taxonomy">
<h3>Control-memory taxonomy<a class="headerlink" href="#control-memory-taxonomy" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#control-memory-taxonomy'">¶</a></h3>
<p>Flynn’s taxonomy is usually applied to machines with von Neumann processors. Further insight may be gained by considering other control mechanisms. The von Neumann machines may be characterized as “control driven”; it is the flow of control represented by the program counter that schedules operations for execution.</p>
<p>“Data-driven” processors schedule operations for execution when their operands become available. In the paradigmatic variety of data-driven machines, scalar data values flow in
tokens over an interconnection network to the instructions that work upon them (hence the term “data flow”). When a token arrives, the hardware checks that all operands are present and, if they are, schedules the instruction for execution. Data-flow machines are easily built distributed memory.</p>
<p>It is also possible to store the data in shared memory and signal each instruction whose operand is now available. Similarly, there is a technique for handling large data structures.</p>
<p>An entire data structure cannot be passed in a token, so the structure is stored in a memory where components of the structure arrive as they are computed. Fetches of the elements arrive in tokens and wait for the value of the element to arrive, whereupon the value is sent on in a token to where the fetch specifies.</p>
<p>A “demand-driven” processor performs computations when values are demanded. For example, when the value of a binary operator is demanded, the operator, in turn, demands the values of its operands. A common implementation of demand-driven processors is based on “reductions,” which occur when a functional program is repeatedly rewritten until the solution is computed. The rewritings include replacing an operator applied to data values with its result and replacing a function call with the function body, with the actual parameters substituted for the formal. Reductions are performed on an internal representation of the program. Two common representations are graphs and strings. Graphs consist of nodes linked together with pointers and hence work best with shared memory. Strings can be spread across a chain of processors so that an individual processor can reduce sub-expressions contained entirely in its memory and neighboring processors can shift expressions falling across their boundary into one of them.“Pattern-driven” computation is typically done without specialized hardware and is implemented atop von Neumann machines. Shared-memory, pattern-driven programming usually means parallel-logic programming. Distributed-memory, pattern-driven programming is represented in this book by Active Messages and Concurrent Aggregates.</p>
</section>
<section id="speedup-vs-efficiency">
<h3>Speedup vs. Efficiency<a class="headerlink" href="#speedup-vs-efficiency" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#speedup-vs-efficiency'">¶</a></h3>
<p>We want to use parallelism to compute answers more quickly. How much more quickly?</p>
<p>We define speedup as</p>
<div class="math notranslate nohighlight">
\[S = \frac{T_1}{T_n}\]</div>
<p>where <span class="math notranslate nohighlight">\(T_1\)</span> is defined as the execution time of the sequential algorithm for the problem on a single processor, and <span class="math notranslate nohighlight">\(T_n\)</span> is the execution time of the parallel algorithm on <code class="docutils literal notranslate"><span class="pre">n</span></code> processors. Notice several things:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(T_n\)</span> should be smaller than <span class="math notranslate nohighlight">\(T_1\)</span> , since the parallel algorithm should run faster than the sequential algorithm.</p></li>
<li><p>The larger the value of S, the better. This is coherent with a cultural metaphor of bigger is better1 (even though we want the smallest run time possible).</p></li>
<li><p><span class="math notranslate nohighlight">\(T_1\)</span> is supposed to be the run time of the best possible sequential algorithm, but in general, the best possible algorithm is an unknown quantity. Thus, it is often the case that :math:<code class="docutils literal notranslate"><span class="pre">T_1</span></code> is simply a version of the parallel program that is run sequentially.</p></li>
<li><p>We define linear speedup as:</p>
<div class="math notranslate nohighlight">
\[S = \frac{T_1}{T_n} = n\]</div>
</li>
</ul>
<p>We would expect that speedup cannot be better (larger) than linear, and indeed should be smaller. If the entire work of the sequential program could be evenly divided among the $n$ processors, they could all complete in $1/n$ the time.
But it is unlikely that the work could be divided evenly; programs tend to have a sequential part, such as initialization or reading data from or writing results to sequential files.
If the sequential part can only be done on a single machine, then only the rest can be run in parallel.
We will examine this in more detail when we discuss Amdahl’s law.
Even if the program could be evenly divided among <span class="math notranslate nohighlight">\(n\)</span> processors, the processors would probably have to coordinate their work with each other, which would require extra instruction executions beyond the sequential program.
Therefore, <span class="math notranslate nohighlight">\(T_n\)</span> may be <span class="math notranslate nohighlight">\(\frac{1}{n}\)</span> of a larger value than <span class="math notranslate nohighlight">\(T_1\)</span>.</p>
<p>Moreover, <span class="math notranslate nohighlight">\(T_1\)</span> is supposed to be the best known sequential algorithm.  If the parallel algorithm runs faster on a single machine, it would be a better sequential algorithm, and therefore, you’d use it. So you can expect the algorithm <span class="math notranslate nohighlight">\(T_1\)</span> to be at least as good as the algorithm for <span class="math notranslate nohighlight">\(T_n\)</span>. You cannot expect any help from differences in the algorithms in achieving even linear speedup.</p>
<p>However, in practice, super-linear speedup is sometimes observed. There are several reasons for this:</p>
<div class="math notranslate nohighlight">
\[S &gt; n\]</div>
<ul class="simple">
<li><p>The hardware is different. The parallel machine has more processors, and hence more cache memory, for one thing. Better locality and pipelining can also play a role.</p></li>
<li><p>The algorithm is different. For example, a depth-first search on a sequential machine might be translated into a collection of depth-first searches on the nodes of a parallel computer, but the parallel depth-first searches would have an element of a breadth-first search. A single depth-first search might spend a large amount of time on one fruitless branch, whereas with several searches, it is likely that another path might find the solution more quickly.</p></li>
</ul>
<p>Efficiency is defined as</p>
<div class="math notranslate nohighlight">
\[E = \frac{S}{n} = \frac{T_1}{n T_n} = \frac{T_1 / n}{T_n}\]</div>
<p>The formula shows two ways to think about efficiency. Suppose you were to run the parallel program on a serial machine. The serial machine would have to execute all the parallel processes. If there are <code class="docutils literal notranslate"><span class="pre">n</span></code> processes, then the serial execution shouldn’t take more than about nTn (assuming that the time to swap the processor from one process to another is negligible). Efficiency, then, would measure the ratio of the actual sequential time to the worst expected time to execute the <code class="docutils literal notranslate"><span class="pre">n</span></code> processes sequentially.</p>
<p>Suppose that, on the other hand, you calculate how long you would expect it to take to run the sequential algorithm on <code class="docutils literal notranslate"><span class="pre">n</span></code> processors, assuming linear speedup. That gives you <code class="docutils literal notranslate"><span class="pre">T_1</span></code>/ n.</p>
<p>The efficiency would be the ratio of execution time with linear speedup to observed execution time. If speedup is no greater than linear, efficiency will be less than or equal to 1.</p>
</section>
<section id="amdahls-law">
<span id="index-10"></span><h3>Amdahl’s Law<a class="headerlink" href="#amdahls-law" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#amdahls-law'">¶</a></h3>
<p>Amdahl’s law does not really deserve the title of law. It is merely a back-of-the-envelope attempt (or conjecture) to prove that there are severe limits to the speedup that can be achieved by a parallel program. Amdahl’s law asserts that there is a serial part of any parallel program that must be executed sequentially, and the time required for this part will be a lower limit on the time the program takes to execute. Consider a serial program that executes in time T. Let’s calculate the best speedup we could achieve if a fraction f of the execution time is taken up by sequential execution. If you divide the parallel execution time into the serial and parallel parts, you get speedup with an upper bound of</p>
<div class="math notranslate nohighlight">
\[E = \frac{T}{f T + \frac{(1 - f)T}{n}}\]</div>
<p>We get this equation by taking the definition of speedup and breaking down Tn into the time taken by the serial fraction (fT) and the time taken by the parallel fraction [(1– f)T]. We divide the parallel fraction by <code class="docutils literal notranslate"><span class="pre">n</span></code> to calculate the best we could expect from a linear speedup.</p>
<p>T appears as a factor in both the numerator and the denominator. Thus, it can be removed, which leads to an equation not involving T, or</p>
<div class="math notranslate nohighlight">
\[S = \frac{1}{f + \frac{(1 - f)}{n}}\]</div>
<p>As <code class="docutils literal notranslate"><span class="pre">n</span></code> approaches infinity (i.e., the number of processors is increased), we arrive at the folllowing limit:</p>
<div class="math notranslate nohighlight">
\[\lim_{x\to\infty} S = \lim_{x\to\infty} \frac{1}{f + \frac{(1 - f)}{n}} = \frac{1}{f}\]</div>
</section>
<section id="scalability">
<h3>Scalability<a class="headerlink" href="#scalability" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#scalability'">¶</a></h3>
<p>A flaw in the reasoning behind Amdahl’s law is that it deals with fixed-sized problems and questions how much faster they can be run. This is not, however, the way massively paral- lel processors are used. Take the example of weather forecasting. The calculations are made by superimposing a mesh onto the atmosphere and calculating pressure, tempera- ture, humidity, etc., at each mesh point, repeatedly using the values at the surrounding points at small time intervals. The more numerous the mesh points and the smaller the time intervals, the better is the forecast. But the more calculations that are required, the
slower the program runs. And for weather forecasting, if the calculation takes too long, it loses all value. When presented with a faster machine, weather forecasters will use more grid points and a smaller step size. They increase the problem size to the largest possible value that allows the answer to be reached in the same amount of time.</p>
<p>Let’s rephrase the calculation, starting with a parallel program with serial fraction g that runs in time R on <code class="docutils literal notranslate"><span class="pre">n</span></code> processors. If we ran the calculation on a single processor, how long would it take? The answer is</p>
<div class="math notranslate nohighlight">
\[T = g R + n(1 - g)R\]</div>
<p>This equation follows, since the serial fraction will still take the same time :math:<code class="docutils literal notranslate"><span class="pre">g</span> <span class="pre">R</span></code> and the <span class="math notranslate nohighlight">\(n\)</span> parts of the parallel fraction <span class="math notranslate nohighlight">\((1-g) R\)</span> would have to be interleaved.</p>
<p>This results in the speedup calculation</p>
<div class="math notranslate nohighlight">
\[S = \frac{g R + n(1 - g)R}{R} = g + n(1 - g)\]</div>
<p>a linear speedup with slope (1 × g). The efficiency is</p>
<div class="math notranslate nohighlight">
\[E = 1 - g \frac{n - 1}{n}\]</div>
<p>which approaches the parallel fraction as the number of processors increases. In this for- mulation, there is no theoretical limit on speedup. As long as we scale the problem size to the size of the machine, we will not run into limits.</p>
<p>Another aspect of this argument against Amdahl’s law is that, as the problem size increases, the serial fraction may decrease. Consider a program that reads in two N-by-N matrices, multiplies them, and writes out the result. The serial I/O time grows as <span class="math notranslate nohighlight">\(N^2\)</span>, while the multiplication, which is highly parallelizable, grows as <span class="math notranslate nohighlight">\(N^3\)</span>.</p>
</section>
<section id="gustafson-s-law-looking-beyond-amdahl-s-law">
<span id="index-11"></span><h3>Gustafson’s Law: Looking Beyond Amdahl’s Law<a class="headerlink" href="#gustafson-s-law-looking-beyond-amdahl-s-law" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#gustafson-s-law-looking-beyond-amdahl-s-law'">¶</a></h3>
<p>Gustafson’s Law, proposed by John Gustafson in 1988, presents a significant
principle in the field of parallel computing, fundamentally differing from
Amdahl’s Law in its approach to evaluating the benefits of parallelization.
This difference has significant merit and is one well worth exploring, even as the ideas of Amdahl’s Law remain of importance. After all, many problems, including intractable ones, cannot run on any number of nodes. (Quantum computing may change that. But we’re not there yet!)</p>
<p>Gustafson’s Law reframes the problem of parallelization by suggesting that
with more processors, we typically choose to solve larger problems, rather
than solving the same problem faster–a view strongly justified by modern practice of distinguishing <em>strong</em> vs. <em>weak</em> scaling (not covered here–yet).</p>
<p>This reflects a realistic approach to
how computational resources are often used in practice. The law posits that
the speedup of a parallel system is linearly proportional to the number of
processors, as it emphasizes the increased problem size that can be handled.
Mathematically, Gustafson’s Law is represented as <span class="math notranslate nohighlight">\(S = B + (1 - B) \times P\)</span>,
where <span class="math notranslate nohighlight">\(S\)</span> is the speedup, <span class="math notranslate nohighlight">\(B\)</span> is the non-parallelizable portion
of the task, and <span class="math notranslate nohighlight">\(P\)</span> is the number of processors.</p>
<p>In contrast to Amdahl’s Law, which focuses on a fixed problem size and analyzes
the speedup achieved by parallelizing parts of the problem, Gustafson’s Law
considers the <em>increase in problem size</em> with more available resources. While Amdahl’s Law
can be construed as more pessimistic about the benefits of parallelization, emphasizing
the bottleneck due to the serial portion of a task, Gustafson’s Law provides a
more optimistic view by shifting focus to the parallelizable portion and its
expansion to available resources. Gustafson’s Law is often considered more reflective of real-world
scenarios, where computing power is used to tackle more extensive and complex
problems over time.</p>
<p>In summary, Amdahl’s Law provides a theoretical limit on the speedup for a
<em>fixed-size problem</em>, but Gustafson’s Law offers a different perspective,
aligning more closely with practical applications and the evolving nature of
computational tasks in parallel computing environments.</p>
</section>
<section id="granularity">
<h3>Granularity<a class="headerlink" href="#granularity" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#granularity'">¶</a></h3>
<p>Grain size loosely refers to the amount of computation that is done between communications or synchronizations. Too large a grain size can result in an unbalanced load. Too small a grain size can waste too much time on system overhead. Consider eight processors that are to execute 10 independent tasks, each of which takes <span class="math notranslate nohighlight">\(t\)</span> time units. Suppose the system takes <span class="math notranslate nohighlight">\(s\)</span> time units to run a task. The schedule looks like this:</p>
<p>Six processors execute one task completing in time <span class="math notranslate nohighlight">\(t + s\)</span>.</p>
<ul class="simple">
<li><p>Two processors execute two tasks completing in time <span class="math notranslate nohighlight">\(2t + 2s\)</span>.</p></li>
<li><p>The overall completion time is the maximum of any processor’s completion time: <span class="math notranslate nohighlight">\(2t + 2s\)</span>.</p></li>
</ul>
<p>Suppose we divide each task into 10 independent tasks, giving us 100 tasks for the entire job. Each task now will take <span class="math notranslate nohighlight">\(\frac{t}{10}\)</span> time units. The schedule now looks like this:</p>
<p>Four processors execute 12 tasks completing at time <span class="math notranslate nohighlight">\(\frac{12t}{10} + 12s\)</span>.
- Four processors execute 13 tasks completing at time <span class="math notranslate nohighlight">\(\frac{13t}{10} + 13s\)</span>.
- The overall completion time is the maximum of any processor’s completion time: <span class="math notranslate nohighlight">\(\frac{13t}{10} + 13s\)</span>.</p>
<p>How do these compare? If s is negligible compared to t, then schedule (1) will complete in 2t, and schedule (2) in 1.3t. However, 13s is significantly larger than 2s, so system over- head s, being even a small fraction of grain size t, might destroy all of the advantages of load balancing. What is the cutover point? That is, at what fraction of t does s cause sched- ule (2) to take as long as schedule (1)? The answer is</p>
<p>2t + 2s = 1.3t + 13s s = (0.7/11)t = 0.064t</p>
<p>So, if s is even seven percent of t, the version with 100 tasks will be as slow as the version with 10. So how do you choose a good grain size? Folklore suggests that one millisecond of execution between communications is a reasonable amount. Other folklore suggests that processing 300–400 array elements between communications is good on some systems. What you will probably have to do is experiment for yourself to find a good grain size. By parameterizing your actual code, you can enable the possibility to experiment.</p>
</section>
<section id="starvation-and-deadlock">
<h3>Starvation and deadlock<a class="headerlink" href="#starvation-and-deadlock" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#starvation-and-deadlock'">¶</a></h3>
<p>Starvation results when some user computations do not get adequate processor time. Here’s an example of starvation on a distributed-memory machine: For some distributed computa- tions, it is difficult to determine if they are finished. There are some algorithms that send system probe messages around to inquire about the state of the computation. Starvation can result if the probe messages use up significant processor time, making processor time unavailable to the user computations. On shared-memory machines, processors lock and unlock resources. When a resource is unlocked, one of the processors waiting for it (if any) is allowed to proceed. If the resource allocation mechanism is unfair, some waiting pro- cesses may be long delayed, while other processes acquire the resource repeatedly.</p>
<p>A set of processes is deadlocked if each process is waiting for resources that other pro- cesses in the set hold and none will release until the processes have been granted the other resources that they are waiting for. There are four conditions required for deadlock:
• Mutual Exclusion: Only a process in possession of a resource may proceed.
• Hold and Wait: Processes will hold resources and wait for others.
• No Preemption: A resource may not be removed from one process to give to another.
• Circular Wait: There exists a cycle of processes holding resources and waiting for resources the next process in the cycle holds.</p>
<p>There are three things you can try to do about deadlock:
• You can try to detect when deadlock has occurred and then try to do something about it. For example, you may cancel one or more of the processes involved to free the resources they hold.Usually, this requires the presence of a monitor process that effec- tively acts as a proxy for any resource request.
• You can try to avoid creating a deadlock by checking before each resource allocation to determine whether the allocation might result in a deadlock and then allowing pro- cesses to proceed only if it is safe to do so.
• You can try to make it impossible for deadlock to occur. The easiest prevention is to eliminate circular waits by numbering the resources and requesting resources in ascending numeric order. That is, never request a resource if you already possess one with a higher number.</p>
</section>
<section id="flooding-throttling">
<h3>Flooding/Throttling<a class="headerlink" href="#flooding-throttling" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#flooding-throttling'">¶</a></h3>
<p>Strangely, one of the problems with parallelism is having too much rather than too little. For many parallel algorithms (especially divide and conquer and combinatoric search), a problem is repeatedly broken into smaller parts that can be run in parallel. Once the num- ber of parallel parts significantly exceeds the number of processors available, it is some- times detrimental to create more parallelism: All processors will be kept busy anyway, the
time to create more parallel tasks will be wasted, and the storage for those task descrip-
tions will tax the parallel machine’s memory.</p>
<p>Preventing a flood of parallelism typically requires extra programming: The algorithm must be broken down into the code that is executed before enough parallel tasks are cre- ated, which creates more tasks, and the code that is executed after sufficient tasks are available, which does its work within a single task.</p>
<p>Choice of when to switch from creating more tasks to executing within tasks can be made statically, before the algorithm runs, or dynamically, in response to the system load. Dynamic switching requires additional information about the current state of the system, which is oftentimes not available or is highly imprecise.</p>
</section>
<section id="layout">
<h3>Layout<a class="headerlink" href="#layout" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#layout'">¶</a></h3>
<p>The layout of a data structure on a distributed-memory machine can make a significant dif- ference in performance. There are two interacting concerns. First, it is important to balance the load so that all nodes have approximately the same amount of work to do. Secondly, it helps to have most communication between neighboring nodes; there won’t be as many queueing delays as messages contend for communication edges along longer paths.
Consider, though, a simulation of the cosmos: If you divide space into equally sized cubes and assign one cube to each node of a multicomputer, then communication of gravitation and movements of mass can be done between neighboring nodes on a mesh-connected computer. Unfortunately, there will be vast regions of nearly empty space mapped to some regions of nodes, while those parts of space with clusters of galaxies will be mapped into other nodes; that is, the load will be horribly imbalanced. A way to balance the load is to divide space into a larger number of regions and randomize their mapping onto the nodes, say, by hashing their coordinates to give the node number. Then you can count on the law of large numbers to balance the load, but communication between neighboring regions is no longer between neighboring nodes.
Suppose we have N rows of an array that must be processed. How can we divide them evenly among P nodes?</p>
<ol class="arabic simple">
<li><p>We could give floor <span class="math notranslate nohighlight">\(N ⁄ P\)</span> rows to each of the first <span class="math notranslate nohighlight">\(P - 1\)</span> nodes and the remaining rows to the last node. If N = 15 and P = 4, nodes 0, 1, and 2 get three rows, and node 3 gets six. The load is imbalanced, and the completion time will be dominated by the last node.</p></li>
<li><p>We could give ceiling <span class="math notranslate nohighlight">\(N ⁄ P\)</span> rows to each of the first <span class="math notranslate nohighlight">\(P - 1\)</span> nodes and the remaining rows to the last. If N = 21 and P = 5, we would assign five rows to each of the first four nodes and one row to the last. The last is underutilized, but it’s not as severe as case (1), where the last node was the bottleneck.</p></li>
</ol>
<p>We could try to assign the rows so that no node has more than one row more than any other node. An easy way to do this is to assign node i all rows j such that j mod P = i, assuming rows are numbered zero through <span class="math notranslate nohighlight">\(N - 1\)</span> and nodes are numbered zero through <span class="math notranslate nohighlight">\(P - 1\)</span>. Node i will contain rows i, i + P, i + 2P, i + 3P, … .
We can assign blocks of rows to nodes, as in (1) and (2), but guarantee that no node has more than one more row than any other node, as in (3). Assign node i the rows in the range Li to Ui inclusive, where</p>
<p>Some algorithms divide arrays into regions that communicate along their edges, so mes- sages will be shorter and faster if the perimeters of regions are smaller: Square regions tend to be better than long, rectangular regions.</p>
</section>
<section id="latency">
<h3>Latency<a class="headerlink" href="#latency" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#latency'">¶</a></h3>
<p>As machines get larger, physical packaging itself requires that components get further apart. Therefore, it will take longer for information to flow between some components rather than others. This implies that the larger, shared-memory machines will be NUMA (nonuniform memory access). Data layout becomes increasingly important. Algorithms may benefit by being rewritten to fetch remote objects, operate on them locally, and then write them back, rather than just manipulating them in place.</p>
<p>Latency is also one of the considerations in laying out tasks and data on distributed-mem- ory machines. On distributed-memory machines, one has the extra option of using asyn- chronous message passing to allow other computations to be performed while messages are being passed.</p>
</section>
<section id="scheduling-needed-for-understanding-q-submit">
<h3>Scheduling [needed for understanding q.submit()]<a class="headerlink" href="#scheduling-needed-for-understanding-q-submit" title="Permalink to this heading" x-intersect.margin.0%.0%.-70%.0%="activeSection = '#scheduling-needed-for-understanding-q-submit'">¶</a></h3>
<p>Scheduling assigns tasks to processors to be run in a particular order or at particular times. There is a large amount of literature devoted to process scheduling, the major import of which is this: Almost any scheduling of activities on processors is an NP-hard problem. For practical purposes, the meaning of NP-hard is this: The worst-case time to run an NP- hard algorithm grows so rapidly (e.g., doubling when you add one element to the input), that you may not get a solution for a modestly sized problem before the sun burns out. Do not seek perfect solutions to NP-hard problems. Instead, look for ways to quickly get solu- tions that are reasonably good most of the time. Static scheduling of tasks on processors would be done before the tasks are run. Dynamic scheduling assigns tasks during execu- tion. Self-scheduling is a form of dynamic scheduling in which the processors themselves select which task to execute next.</p>
<p>The techniques for partitioning rows among nodes that we saw in the discussion of layout are also applicable to processor scheduling on shared-memory machines. For technique (3), an easy way to assign process i all rows j such that j mod P = i is to handle the rows in a loop:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="p">(</span><span class="n">j</span> <span class="o">=</span> <span class="n">my_id</span><span class="p">;</span> <span class="n">j</span><span class="o">&lt;</span><span class="n">n</span><span class="p">;</span> <span class="n">j</span> <span class="o">+=</span><span class="n">P</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">process</span> <span class="n">row</span> <span class="n">j</span>
<span class="p">}</span>
<span class="n">where</span> <span class="n">the</span> <span class="n">rows</span> <span class="n">are</span> <span class="n">numbered</span> <span class="mi">0</span> <span class="n">through</span> <span class="n">N</span><span class="o">-</span><span class="mi">1</span>
<span class="n">my_id</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">node</span> <span class="n">number</span> <span class="ow">in</span> <span class="n">the</span> <span class="nb">range</span> <span class="mf">0.</span><span class="o">.</span><span class="n">P</span><span class="o">-</span><span class="mi">1</span>
</pre></div>
</div>
<p>Rather than assign entire rows or columns to processors, better load balancing can some- times be accomplished by assigning groups of elements. If there are K total elements in an array, we can assign them numbers 0 through <span class="math notranslate nohighlight">\(K - 1\)</span>, assign ranges of those numbers to processors, and convert from the element number to the array indices when necessary. For an M × N matrix A with zero origin addressing, element A[i,j] would be given the num- ber i*N+j in row major order. Similarly, the element with number q would correspond to A[i,j], where</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">floor</span><span class="p">(</span><span class="n">q</span><span class="o">/</span><span class="n">N</span><span class="p">)</span>
<span class="n">j</span> <span class="o">=</span> <span class="n">q</span> <span class="n">mod</span> <span class="n">N</span>
</pre></div>
</div>
<p>A simple form of self-scheduling for K elements is to keep the index C of the next element to be processed and to allocate items by incrementing or decrementing C. The following code is illustrative:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">initially</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">K</span><span class="o">-</span><span class="mi">1</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="k">while</span> <span class="p">(</span><span class="n">i</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
   <span class="n">lock</span> <span class="n">C_lock</span><span class="p">;</span>
   <span class="n">i</span> <span class="o">=</span> <span class="n">C</span><span class="p">;</span>
   <span class="n">C</span> <span class="o">=</span> <span class="n">C</span><span class="o">-</span><span class="mi">1</span><span class="p">;</span>
   <span class="n">unlock</span> <span class="n">C_lock</span><span class="p">;</span>
   <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="n">process</span> <span class="n">item</span> <span class="n">i</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>However, if the processing of a single element takes little time, the grain size is too small. Of course the processor could allocate some constant number of elements greater than one at a time. This is clearly better for grain size, but may still have load balance problems. An improved self-scheduling algorithm has each of the P processors allocate ceiling(C/P) elements (i.e., allocate 1/P of the remaining elements):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">initially</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">K</span>
<span class="n">low</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="k">while</span> <span class="p">(</span><span class="n">low</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
   <span class="n">lock</span> <span class="n">C_lock</span><span class="p">;</span>
   <span class="n">t</span> <span class="o">=</span> <span class="n">ceiling</span><span class="p">(</span><span class="n">C</span><span class="o">/</span><span class="n">P</span><span class="p">);</span>
   <span class="k">if</span> <span class="p">(</span><span class="n">t</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">low</span><span class="o">=-</span><span class="mi">1</span><span class="p">;</span>
   <span class="k">else</span> <span class="p">{</span>
   <span class="n">high</span> <span class="o">=</span> <span class="n">C</span><span class="o">-</span><span class="mi">1</span><span class="p">;</span>
   <span class="n">low</span> <span class="o">=</span> <span class="n">C</span> <span class="o">=</span> <span class="n">C</span><span class="o">-</span><span class="n">t</span><span class="p">;</span> <span class="p">}</span>
   <span class="n">unlock</span> <span class="n">C_lock</span><span class="p">;</span>
   <span class="k">if</span> <span class="p">(</span><span class="n">low</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">)</span>
       <span class="n">process</span> <span class="n">items</span> <span class="n">low</span> <span class="n">through</span> <span class="n">high</span> <span class="n">inclusive</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
</section>
</div></div><aside class="hidden text-sm xl:block" id="right-sidebar">
<div class="sticky top-16 -mt-10 max-h-[calc(var(--vh)-4rem)] overflow-y-auto pt-6 space-y-2"><p class="font-medium">On this page</p>
<ul>
<li><a :data-current="activeSection === '#notable-early-parallel-computing-systems'" class="reference internal" href="#notable-early-parallel-computing-systems">Notable Early Parallel Computing Systems</a><ul>
<li><a :data-current="activeSection === '#cluster-computing'" class="reference internal" href="#cluster-computing">Cluster Computing</a></li>
<li><a :data-current="activeSection === '#accelerators'" class="reference internal" href="#accelerators">Accelerators</a></li>
<li><a :data-current="activeSection === '#vector-machines-cray'" class="reference internal" href="#vector-machines-cray">Vector machines (Cray)</a></li>
<li><a :data-current="activeSection === '#connection-machine-thinking-machines'" class="reference internal" href="#connection-machine-thinking-machines">Connection Machine (Thinking Machines)</a></li>
<li><a :data-current="activeSection === '#systolic-architectures'" class="reference internal" href="#systolic-architectures">Systolic architectures</a></li>
<li><a :data-current="activeSection === '#loop-parallelism-in-fortran'" class="reference internal" href="#loop-parallelism-in-fortran">Loop parallelism in FORTRAN</a></li>
<li><a :data-current="activeSection === '#c-and-data-parallel-c-efforts'" class="reference internal" href="#c-and-data-parallel-c-efforts">C* and Data-Parallel C efforts</a></li>
<li><a :data-current="activeSection === '#index-7'" class="reference internal" href="#index-7">CUDA</a></li>
<li><a :data-current="activeSection === '#openmp'" class="reference internal" href="#openmp">OpenMP</a></li>
</ul>
</li>
<li><a :data-current="activeSection === '#parallel-concepts-for-humans'" class="reference internal" href="#parallel-concepts-for-humans">Parallel Concepts for Humans</a><ul>
<li><a :data-current="activeSection === '#von-neumann-machines-and-their-limits'" class="reference internal" href="#von-neumann-machines-and-their-limits">von Neumann machines and their limits</a></li>
<li><a :data-current="activeSection === '#flynn-s-taxonomy'" class="reference internal" href="#flynn-s-taxonomy">Flynn’s taxonomy</a></li>
<li><a :data-current="activeSection === '#control-memory-taxonomy'" class="reference internal" href="#control-memory-taxonomy">Control-memory taxonomy</a></li>
<li><a :data-current="activeSection === '#speedup-vs-efficiency'" class="reference internal" href="#speedup-vs-efficiency">Speedup vs. Efficiency</a></li>
<li><a :data-current="activeSection === '#amdahls-law'" class="reference internal" href="#amdahls-law">Amdahl’s Law</a></li>
<li><a :data-current="activeSection === '#scalability'" class="reference internal" href="#scalability">Scalability</a></li>
<li><a :data-current="activeSection === '#gustafson-s-law-looking-beyond-amdahl-s-law'" class="reference internal" href="#gustafson-s-law-looking-beyond-amdahl-s-law">Gustafson’s Law: Looking Beyond Amdahl’s Law</a></li>
<li><a :data-current="activeSection === '#granularity'" class="reference internal" href="#granularity">Granularity</a></li>
<li><a :data-current="activeSection === '#starvation-and-deadlock'" class="reference internal" href="#starvation-and-deadlock">Starvation and deadlock</a></li>
<li><a :data-current="activeSection === '#flooding-throttling'" class="reference internal" href="#flooding-throttling">Flooding/Throttling</a></li>
<li><a :data-current="activeSection === '#layout'" class="reference internal" href="#layout">Layout</a></li>
<li><a :data-current="activeSection === '#latency'" class="reference internal" href="#latency">Latency</a></li>
<li><a :data-current="activeSection === '#scheduling-needed-for-understanding-q-submit'" class="reference internal" href="#scheduling-needed-for-understanding-q-submit">Scheduling [needed for understanding q.submit()]</a></li>
</ul>
</li>
</ul>
</div>
</aside>
</main>
</div>
</div><footer class="py-6 border-t border-border md:py-0">
<div class="container flex flex-col items-center justify-between gap-4 md:h-24 md:flex-row">
<div class="flex flex-col items-center gap-4 px-8 md:flex-row md:gap-2 md:px-0">
<p class="text-sm leading-loose text-center text-muted-foreground md:text-left">© 2013-2019, UnoAPI Software Systems Laboratory Built with <a class="font-medium underline underline-offset-4" href="https://www.sphinx-doc.org" rel="noreferrer">Sphinx 6.2.1</a></p>
</div>
</div>
</footer>
</div>
</body>
</html>